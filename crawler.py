#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é“¶è¡Œæ‹›è˜ä¿¡æ¯çˆ¬è™«

åŠŸèƒ½ï¼š
1. çˆ¬å–é“¶è¡Œæ‹›è˜åˆ—è¡¨é¡µé¢
2. æå–æ¯ä¸ªèŒä½çš„è¯¦ç»†ä¿¡æ¯
3. æ£€æµ‹æ–°å¢èŒä½å¹¶æ¨é€é€šçŸ¥
4. æ”¯æŒå®šæ—¶ä»»åŠ¡è¿è¡Œ
"""

import os
import json
import time
import logging
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class BankJobCrawler:
    """é“¶è¡Œæ‹›è˜ä¿¡æ¯çˆ¬è™«ç±»"""
    
    def __init__(self):
        self.base_url = os.getenv('BASE_URL', 'http://www.yinhangzhaopin.com')
        self.list_url = os.getenv('LIST_URL', 'http://www.yinhangzhaopin.com/tag/shehuizhaopin_13698_1.html')
        self.data_file = os.getenv('DATA_FILE', 'data/jobs_history.json')
        self.backup_file = os.getenv('BACKUP_FILE', 'data/jobs_backup.txt')
        self.log_file = os.getenv('LOG_FILE', 'logs/crawler.log')
        # æ”¯æŒå¤šä¸ªServeré…±å¯†é’¥
        server_chan_keys_str = os.getenv('SERVER_CHAN_KEY', '')
        if server_chan_keys_str:
            # æ”¯æŒé€—å·æˆ–åˆ†å·åˆ†éš”çš„å¤šä¸ªå¯†é’¥
            if ',' in server_chan_keys_str:
                self.server_chan_keys = [key.strip() for key in server_chan_keys_str.split(',') if key.strip()]
            elif ';' in server_chan_keys_str:
                self.server_chan_keys = [key.strip() for key in server_chan_keys_str.split(';') if key.strip()]
            else:
                # å•ä¸ªå¯†é’¥
                self.server_chan_keys = [server_chan_keys_str.strip()] if server_chan_keys_str.strip() else []
        else:
            self.server_chan_keys = []
        
        # è¯·æ±‚é…ç½®
        self.request_delay = float(os.getenv('REQUEST_DELAY', '1'))
        self.timeout = int(os.getenv('TIMEOUT', '30'))
        self.retry_times = int(os.getenv('RETRY_TIMES', '3'))
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self._create_directories()
        
        # åŠ è½½å†å²æ•°æ®
        self.jobs_history = self._load_history()
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        os.makedirs(os.path.dirname(self.data_file), exist_ok=True)
        os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
    
    def _load_history(self) -> Dict:
        """åŠ è½½å†å²æ•°æ®"""
        if os.path.exists(self.data_file):
            try:
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.error(f"åŠ è½½å†å²æ•°æ®å¤±è´¥: {e}")
                return {}
        return {}
    
    def _save_history(self):
        """ä¿å­˜å†å²æ•°æ®"""
        try:
            with open(self.data_file, 'w', encoding='utf-8') as f:
                json.dump(self.jobs_history, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ä¿å­˜å†å²æ•°æ®å¤±è´¥: {e}")
    
    def _save_jobs_backup(self, new_jobs: List[Dict]):
        """ä¿å­˜æ–°èŒä½åˆ°å¤‡ä»½æ–‡ä»¶ï¼ˆæ–°å†…å®¹åœ¨é¡¶éƒ¨ï¼‰"""
        if not new_jobs:
            return
        
        try:
            # è¯»å–ç°æœ‰å†…å®¹
            existing_content = ""
            if os.path.exists(self.backup_file):
                with open(self.backup_file, 'r', encoding='utf-8') as f:
                    existing_content = f.read()
            
            # æ ¼å¼åŒ–æ–°èŒä½ä¿¡æ¯
            new_content_lines = []
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            new_content_lines.append(f"=== æ›´æ–°æ—¶é—´: {current_time} ===")
            new_content_lines.append(f"æ–°å¢èŒä½æ•°é‡: {len(new_jobs)}")
            new_content_lines.append("")
            
            for i, job in enumerate(new_jobs, 1):
                new_content_lines.append(f"ã€èŒä½ {i}ã€‘")
                new_content_lines.append(f"æ ‡é¢˜: {job.get('title', 'æœªçŸ¥')}")
                new_content_lines.append(f"å…¬å¸: {job.get('company', 'æœªçŸ¥')}")
                new_content_lines.append(f"åœ°ç‚¹: {job.get('location', 'æœªçŸ¥')}")
                new_content_lines.append(f"é“¾æ¥: {job.get('url', 'æœªçŸ¥')}")
                new_content_lines.append("")
                
                # æ·»åŠ èŒä½è¯¦æƒ…
                details = job.get('details', 'æš‚æ— è¯¦æƒ…')
                if details and details != 'æš‚æ— è¯¦æƒ…':
                    new_content_lines.append("èŒä½è¯¦æƒ…:")
                    new_content_lines.append("-" * 50)
                    new_content_lines.append(details)
                    new_content_lines.append("-" * 50)
                else:
                    new_content_lines.append("èŒä½è¯¦æƒ…: æš‚æ— è¯¦æƒ…")
                
                new_content_lines.append("")
                new_content_lines.append("=" * 80)
                new_content_lines.append("")
            
            # å°†æ–°å†…å®¹å†™å…¥æ–‡ä»¶é¡¶éƒ¨
            new_content = "\n".join(new_content_lines)
            
            # å¦‚æœæœ‰ç°æœ‰å†…å®¹ï¼Œåœ¨æ–°å†…å®¹åæ·»åŠ åˆ†éš”ç¬¦å’Œç°æœ‰å†…å®¹
            if existing_content.strip():
                final_content = new_content + "\n" + existing_content
            else:
                final_content = new_content
            
            # å†™å…¥æ–‡ä»¶
            with open(self.backup_file, 'w', encoding='utf-8') as f:
                f.write(final_content)
            
            self.logger.info(f"å¤‡ä»½æ–‡ä»¶å·²æ›´æ–°: {self.backup_file}ï¼Œæ–°å¢ {len(new_jobs)} ä¸ªèŒä½")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¤‡ä»½æ–‡ä»¶å¤±è´¥: {e}")
    
    def _make_request(self, url: str, encoding: str = 'gbk') -> Optional[BeautifulSoup]:
        """å‘é€HTTPè¯·æ±‚å¹¶è¿”å›BeautifulSoupå¯¹è±¡"""
        for attempt in range(self.retry_times):
            try:
                self.logger.info(f"è¯·æ±‚URL: {url} (å°è¯• {attempt + 1}/{self.retry_times})")
                response = requests.get(url, headers=self.headers, timeout=self.timeout)
                response.encoding = encoding
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'lxml')
                time.sleep(self.request_delay)  # è¯·æ±‚é—´éš”
                return soup
                
            except Exception as e:
                self.logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.retry_times}): {e}")
                if attempt < self.retry_times - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    self.logger.error(f"è¯·æ±‚æœ€ç»ˆå¤±è´¥: {url}")
        
        return None
    
    def extract_job_list(self) -> List[Dict]:
        """æå–æ‹›è˜åˆ—è¡¨ä¿¡æ¯"""
        soup = self._make_request(self.list_url)
        if not soup:
            return []
        
        jobs = []
        
        # æ ¹æ®HTMLç»“æ„æå–èŒä½ä¿¡æ¯
        # ä»ç¤ºä¾‹HTMLå¯ä»¥çœ‹å‡ºï¼ŒèŒä½é“¾æ¥åœ¨ <dt><a href="..." title="...">...</a></dt> ç»“æ„ä¸­
        job_links = soup.find_all('dt')
        
        for dt in job_links:
            link = dt.find('a')
            if not link or not link.get('href'):
                continue
            
            href = link.get('href')
            title = link.get('title', '').strip()
            
            # è·³è¿‡éèŒä½é“¾æ¥
            if not href.endswith('.htm') or not title:
                continue
            
            # æ„é€ å®Œæ•´URL
            full_url = urljoin(self.base_url, href)
            
            # è§£æåŸºæœ¬ä¿¡æ¯
            job_info = self._parse_job_basic_info(title, full_url)
            if job_info:
                jobs.append(job_info)
        
        self.logger.info(f"æå–åˆ° {len(jobs)} ä¸ªèŒä½ä¿¡æ¯")
        return jobs
    
    def _parse_job_basic_info(self, title: str, url: str) -> Optional[Dict]:
        """è§£æèŒä½åŸºæœ¬ä¿¡æ¯"""
        try:
            # è§£ææ ‡é¢˜æ ¼å¼: [åœ°åŒº]å¹´ä»½+é“¶è¡Œåç§°+æ‹›è˜å…¬å‘Š(æ—¥æœŸ)
            # ä¾‹å¦‚: [æ¹–åŒ—]2025å¹´æ¹–åŒ—é“¶è¡Œæ€»è¡Œéƒ¨å®¤ç¤¾ä¼šæ‹›è˜å…¬å‘Šï¼ˆ6.27ï¼‰
            
            parts = title.split(']', 1)
            if len(parts) != 2:
                return None
            
            location = parts[0].replace('[', '').strip()
            remaining = parts[1].strip()
            
            # æå–æ—¥æœŸï¼ˆå¦‚æœæœ‰ï¼‰
            date_info = ''
            if 'ï¼ˆ' in remaining and 'ï¼‰' in remaining:
                date_start = remaining.rfind('ï¼ˆ')
                date_end = remaining.rfind('ï¼‰')
                if date_start < date_end:
                    date_info = remaining[date_start+1:date_end]
                    remaining = remaining[:date_start].strip()
            
            # ç”Ÿæˆå”¯ä¸€ID
            job_id = self._generate_job_id(url)
            
            return {
                'id': job_id,
                'title': title,
                'location': location,
                'company': self._extract_company_name(remaining),
                'url': url,
                'date_info': date_info,
                'crawl_time': datetime.now().isoformat(),
                'details': None  # è¯¦æƒ…å°†åœ¨åç»­è·å–
            }
        
        except Exception as e:
            self.logger.warning(f"è§£æèŒä½åŸºæœ¬ä¿¡æ¯å¤±è´¥: {title}, é”™è¯¯: {e}")
            return None
    
    def _extract_company_name(self, text: str) -> str:
        """ä»æ ‡é¢˜ä¸­æå–å…¬å¸åç§°"""
        # ç®€å•çš„å…¬å¸åç§°æå–é€»è¾‘ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µä¼˜åŒ–
        if 'é“¶è¡Œ' in text:
            # æŸ¥æ‰¾åŒ…å«"é“¶è¡Œ"çš„éƒ¨åˆ†
            words = text.split()
            for word in words:
                if 'é“¶è¡Œ' in word:
                    return word
        return text.split()[0] if text.split() else 'æœªçŸ¥'
    
    def _generate_job_id(self, url: str) -> str:
        """æ ¹æ®URLç”Ÿæˆå”¯ä¸€çš„èŒä½ID"""
        # ä»URLä¸­æå–æ–‡ä»¶åä½œä¸ºID
        parsed = urlparse(url)
        path_parts = parsed.path.split('/')
        if path_parts:
            filename = path_parts[-1]
            return filename.replace('.htm', '').replace('.html', '')
        return str(hash(url))
    
    def get_job_details(self, job: Dict) -> Dict:
        """è·å–èŒä½è¯¦ç»†ä¿¡æ¯"""
        soup = self._make_request(job['url'])
        if not soup:
            return job
        
        try:
            # æå–èŒä½è¯¦æƒ…å†…å®¹
            # æ ¹æ®å®é™…HTMLç»“æ„ï¼Œè¯¦æƒ…åœ¨classä¸º'newstxt'çš„divä¸­
            content_div = soup.find('div', class_='newstxt')
            
            if not content_div:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                content_div = soup.find('div', class_='content') or soup.find('div', {'id': 'content'})
                if not content_div:
                    content_div = soup.find('div', class_='article-content') or soup.find('div', class_='job-content')
            
            if content_div:
                # ç§»é™¤å¹¿å‘Šå’Œæ— å…³å†…å®¹
                # ç§»é™¤scriptæ ‡ç­¾
                for script in content_div.find_all('script'):
                    script.decompose()
                
                # ç§»é™¤å¹¿å‘Šç›¸å…³çš„div
                for ad_div in content_div.find_all('div', class_=['yindao', 'zhezhao']):
                    ad_div.decompose()
                
                # ç§»é™¤åˆ†äº«ç›¸å…³å†…å®¹
                for share_div in content_div.find_all('div', {'id': 'ckepop'}):
                    share_div.decompose()
                
                # ç§»é™¤å…è´£å£°æ˜ç­‰
                for disclaimer in content_div.find_all('div', style=lambda x: x and 'color:#666' in x):
                    disclaimer.decompose()
                
                # æ¸…ç†HTMLæ ‡ç­¾ï¼Œæå–çº¯æ–‡æœ¬
                details = content_div.get_text(separator='\n', strip=True)
                
                # è¿›ä¸€æ­¥æ¸…ç†æ–‡æœ¬
                lines = details.split('\n')
                cleaned_lines = []
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('åˆ†äº«åˆ°:') and not line.startswith('è”ç³»æˆ‘ä»¬æ—¶'):
                        cleaned_lines.append(line)
                
                job['details'] = '\n'.join(cleaned_lines)
            else:
                job['details'] = 'æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯'
            
            self.logger.info(f"è·å–èŒä½è¯¦æƒ…æˆåŠŸ: {job['title']}")
            
        except Exception as e:
            self.logger.error(f"è·å–èŒä½è¯¦æƒ…å¤±è´¥: {job['title']}, é”™è¯¯: {e}")
            job['details'] = f'è·å–è¯¦æƒ…æ—¶å‡ºé”™: {e}'
        
        return job
    
    def check_new_jobs(self, current_jobs: List[Dict]) -> List[Dict]:
        """æ£€æŸ¥æ–°å¢çš„èŒä½"""
        new_jobs = []
        
        for job in current_jobs:
            job_id = job['id']
            if job_id not in self.jobs_history:
                new_jobs.append(job)
                self.jobs_history[job_id] = job
        
        if new_jobs:
            self.logger.info(f"å‘ç° {len(new_jobs)} ä¸ªæ–°èŒä½")
        else:
            self.logger.info("æ²¡æœ‰å‘ç°æ–°èŒä½")
        
        return new_jobs
    
    def _format_job_details_markdown(self, job: Dict) -> str:
        """å°†èŒä½è¯¦æƒ…æ ¼å¼åŒ–ä¸ºmarkdown"""
        details = job.get('details', 'æš‚æ— è¯¦ç»†ä¿¡æ¯')
        
        # åŸºæœ¬ä¿¡æ¯
        markdown_content = []
        markdown_content.append(f"## ğŸ“‹ {job.get('title', 'æœªçŸ¥èŒä½')}")
        markdown_content.append("")
        markdown_content.append(f"**ğŸ¢ æ‹›è˜å•ä½ï¼š** {job.get('company', 'æœªçŸ¥å•ä½')}")
        markdown_content.append(f"**ğŸ“ å·¥ä½œåœ°ç‚¹ï¼š** {job.get('location', 'æœªçŸ¥åœ°åŒº')}")
        markdown_content.append(f"**ğŸ“… å‘å¸ƒæ—¥æœŸï¼š** {job.get('date', 'æœªçŸ¥æ—¥æœŸ')}")
        markdown_content.append("")
        
        # è¯¦ç»†ä¿¡æ¯
        if details and details != 'æš‚æ— è¯¦ç»†ä¿¡æ¯':
            markdown_content.append("## ğŸ“ èŒä½è¯¦æƒ…")
            markdown_content.append("")
            
            # æ¸…ç†å’Œæ ¼å¼åŒ–è¯¦æƒ…å†…å®¹
            cleaned_details = details.strip()
            
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = [p.strip() for p in cleaned_details.split('\n') if p.strip()]
            
            for paragraph in paragraphs:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œï¼ˆé€šå¸¸åŒ…å«ç‰¹å®šå…³é”®è¯ï¼‰
                if any(keyword in paragraph for keyword in ['æ‹›è˜', 'å²—ä½', 'æ¡ä»¶', 'è¦æ±‚', 'èŒè´£', 'å¾…é‡', 'ç¦åˆ©', 'è”ç³»']):
                    markdown_content.append(f"### {paragraph}")
                else:
                    markdown_content.append(paragraph)
                markdown_content.append("")
        
        # æ·»åŠ é“¾æ¥
        if job.get('url'):
            markdown_content.append("---")
            markdown_content.append(f"ğŸ”— [æŸ¥çœ‹è¯¦æƒ…]({job['url']})")
        
        return "\n".join(markdown_content)
    
    def send_notification(self, new_jobs: List[Dict]):
        """å‘é€æ–°èŒä½é€šçŸ¥"""
        if not new_jobs or not self.server_chan_keys:
            if not self.server_chan_keys:
                self.logger.warning("æœªé…ç½®Serveré…±å¯†é’¥ï¼Œè·³è¿‡é€šçŸ¥å‘é€")
            return
        
        self.logger.info(f"å‡†å¤‡å‘ {len(self.server_chan_keys)} ä¸ªæ¥æ”¶è€…å‘é€é€šçŸ¥")
        
        try:
            # ä¸ºæ¯ä¸ªæ–°èŒä½å‘é€å•ç‹¬çš„é€šçŸ¥
            for job in new_jobs:
                # æ„é€ é€šçŸ¥å†…å®¹
                title = job.get('title', 'æœªçŸ¥èŒä½')      # title: å¯¹åº”çˆ¬å–åˆ°çš„title
                short = job.get('location', 'æœªçŸ¥åœ°åŒº')  # short: å¯¹åº”çˆ¬å–åˆ°çš„location
                desp = self._format_job_details_markdown(job)  # desp: æ ¼å¼åŒ–çš„å²—ä½è¯¦æƒ…
                
                # å‘æ¯ä¸ªé…ç½®çš„å¯†é’¥å‘é€é€šçŸ¥
                for i, server_chan_key in enumerate(self.server_chan_keys, 1):
                    try:
                        # å‘é€Serveré…±é€šçŸ¥
                        url = f"https://sctapi.ftqq.com/{server_chan_key}.send"
                        data = {
                            'title': title,
                            'short': short,
                            'desp': desp
                        }
                        
                        response = requests.post(url, data=data, timeout=10)
                        response.raise_for_status()
                        
                        self.logger.info(f"é€šçŸ¥å‘é€æˆåŠŸ (æ¥æ”¶è€…{i}): {title} - {short}")
                        
                    except Exception as e:
                        self.logger.error(f"å‘æ¥æ”¶è€…{i}å‘é€é€šçŸ¥å¤±è´¥: {title} - {e}")
                    
                    # é¿å…é¢‘ç¹è¯·æ±‚ï¼Œæ·»åŠ å»¶è¿Ÿ
                    time.sleep(0.5)
                
                # æ¯ä¸ªèŒä½å‘é€å®Œæˆåç¨ä½œå»¶è¿Ÿ
                time.sleep(1)
            
        except Exception as e:
            self.logger.error(f"å‘é€é€šçŸ¥è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    def _format_notification_content(self, jobs: List[Dict]) -> str:
        """æ ¼å¼åŒ–é€šçŸ¥å†…å®¹"""
        content_lines = []
        
        for job in jobs[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ªèŒä½
            lines = [
                f"**{job['title']}**",
                f"- å•ä½: {job['company']}",
                f"- åœ°ç‚¹: {job['location']}",
                f"- é“¾æ¥: {job['url']}",
                ""
            ]
            content_lines.extend(lines)
        
        if len(jobs) > 10:
            content_lines.append(f"... è¿˜æœ‰ {len(jobs) - 10} ä¸ªèŒä½")
        
        return "\n".join(content_lines)
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        self.logger.info("å¼€å§‹è¿è¡Œé“¶è¡Œæ‹›è˜çˆ¬è™«")
        
        try:
            # 1. è·å–èŒä½åˆ—è¡¨
            jobs = self.extract_job_list()
            if not jobs:
                self.logger.warning("æœªè·å–åˆ°ä»»ä½•èŒä½ä¿¡æ¯")
                return
            
            # 2. æ£€æŸ¥æ–°èŒä½
            new_jobs = self.check_new_jobs(jobs)
            
            # 3. è·å–æ–°èŒä½çš„è¯¦ç»†ä¿¡æ¯
            for job in new_jobs:
                self.get_job_details(job)
                self.jobs_history[job['id']] = job  # æ›´æ–°å†å²è®°å½•
            
            # 4. ä¿å­˜å†å²æ•°æ®
            self._save_history()
            
            # 5. ä¿å­˜æ–°èŒä½åˆ°å¤‡ä»½æ–‡ä»¶
            if new_jobs:
                self._save_jobs_backup(new_jobs)
            
            # 6. å‘é€é€šçŸ¥
            if new_jobs:
                self.send_notification(new_jobs)
            
            self.logger.info(f"çˆ¬è™«è¿è¡Œå®Œæˆï¼Œå¤„ç†äº† {len(jobs)} ä¸ªèŒä½ï¼Œæ–°å¢ {len(new_jobs)} ä¸ª")
            
        except Exception as e:
            self.logger.error(f"çˆ¬è™«è¿è¡Œå‡ºé”™: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    crawler = BankJobCrawler()
    crawler.run()

if __name__ == '__main__':
    main()